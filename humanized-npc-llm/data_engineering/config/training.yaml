# Fine-Tuning Configuration

# model:
#   name: "Qwen/Qwen2.5-3B-Instruct"
#   load_in_4bit: true

# lora:
#   rank: 16
#   alpha: 32
#   target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
#   dropout: 0.05

# training:
#   epochs: 3
#   batch_size: 4
#   gradient_accumulation_steps: 4
#   learning_rate: 0.0002
#   max_seq_length: 512
#   warmup_ratio: 0.03
#   weight_decay: 0.01
  
#   output_dir: "outputs/model"
#   logging_steps: 10
#   save_steps: 500
#   eval_steps: 500


# Configuration for the fine-tuning pipeline (Task 2)
# Owner: Subodh Kant

# --- Model Configuration ---
# Base Model: We're using Phi-3-mini (3.8B) as it fits 3-4B
# requirement and is a top-performing SLM. Unsloth has an optimized version.
base_model_id: "Qwen/Qwen2.5-3B-Instruct"

# Output directory for the final LoRA adapters
output_dir: "./outputs/model/"

# --- QLoRA Configuration (PEFT) ---
# From your project plan (Section 2.3)
lora_r: 16
lora_alpha: 32 # 2 * r
lora_dropout: 0.05
# Target modules for Phi-3. Unsloth handles this well automatically,
# but we specify for clarity.
lora_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj"]

# --- Training Hyperparameters ---
# From your project plan (Section 2.3)
num_train_epochs: 3
batch_size: 4
gradient_accumulation_steps: 4 # Effective batch size = 16
learning_rate: 2.0e-4
max_seq_length: 2048 # Increased from 512 to handle rich personas
optimizer: "paged_adamw_8bit" # As specified
lr_scheduler_type: "cosine"
warmup_ratio: 0.03
weight_decay: 0.01
max_grad_norm: 1.0
mixed_precision: "bf16" # bf16 for modern GPUs, fp16 as fallback
gradient_checkpointing: true

# --- Dataset Paths ---
# These paths point to the outputs from Task 1
train_data_file: "../data_engineering/outputs/train.jsonl"
val_data_file: "../data_engineering/outputs/val.jsonl"

# --- Logging ---
# We'll use Weights & Biases for experiment tracking
wandb_project: "humanized-npc-llm"
wandb_run_name: "Qwen2.5-3B-Instruct-v1"
logging_steps: 10
eval_steps: 250 # Evaluate every 250 steps
save_steps: 250 # Save checkpoints every 250 steps
save_total_limit: 2 # Keep only the best and latest
