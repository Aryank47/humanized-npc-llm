# --- Model Configuration ---
base_model_id: "Qwen/Qwen2.5-3B-Instruct"
output_dir: "./outputs/model/"

# --- QLoRA Configuration (PEFT) ---
lora_r: 16
# A user-selected rank for the low-rank adaptation. Lower or higher ranks can be tried depending on compute/memory vs downstream performance. (LoRA supports low-rank updates.)  [oai_citation:1‡arXiv](https://arxiv.org/abs/2106.09685)

lora_alpha: 32  # α = 2 * r
# Scaling factor for the LoRA update. This is a common heuristic scaling, often used in LoRA/QLoRA implementations.  [oai_citation:2‡magazine.sebastianraschka.com](https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch)

lora_dropout: 0.05

lora_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj"]
# Adapter placement: apply LoRA to all linear layers (attention + MLP/feed-forward) — this matches the “all-linear-layers” strategy used in QLoRA.  [oai_citation:3‡arXiv](https://arxiv.org/abs/2305.14314)

# --- Training Hyperparameters ---
num_train_epochs: 3
batch_size: 2
gradient_accumulation_steps: 8  # effective total batch size = 16

learning_rate: 2.0e-4
# Base learning rate — a typical starting point for PEFT/QLoRA fine-tuning (subject to tuning).  [oai_citation:4‡Modal](https://modal.com/blog/lora-qlora)

max_seq_length: 1024
# Context length — sufficient for prompts, dialogue history, and generation context.

optimizer: "paged_adamw_8bit"
# Memory-efficient optimizer suitable when using 4-bit quantization + LoRA adapters. (As in QLoRA.)  [oai_citation:5‡arXiv](https://arxiv.org/abs/2305.14314)

lr_scheduler_type: "cosine"
warmup_ratio: 0.03
# Warmup + scheduler — common fine-tuning heuristics. Their optimal values may vary depending on model, data, and compute.

weight_decay: 0.0
# Since only adapter weights are updated (base model frozen), heavy regularization may not be necessary. Often weight decay is set low or zero in adapter-based fine-tuning.

max_grad_norm: 1.0
# Gradient clipping — good practice to avoid exploding gradients.

mixed_precision: "bf16"
# Use mixed precision (bf16) if hardware supports it, to reduce memory usage. Common in quantized + PEFT setups.  [oai_citation:6‡arXiv](https://arxiv.org/abs/2305.14314)

gradient_checkpointing: true
# Enables checkpointing to save memory during backprop — useful when GPU memory is constrained.

# --- Dataset Paths ---
train_data_file: "../data_engineering/data/processed/v2/train.jsonl"
val_data_file: "../data_engineering/data/processed/v2/val.jsonl"

# --- Logging & Checkpointing ---
wandb_project: "humanized-npc-llm"
wandb_run_name: "Qwen2.5-3B-Full-Dataset-v1"

logging_steps: 10
eval_steps: 200
save_steps: 200
save_total_limit: 3